"""
Automatic News Fetcher - Multi-source News Crawler
æ¯æ—¥è‡ªåŠ¨è·å–8ä¸ªå“ç‰Œçš„æœ€æ–°èµ„è®¯ï¼ˆæ¯ä¸ªå“ç‰Œè‡³å°‘5æ¡ï¼‰
æ”¯æŒå¤šä¸ªæ•°æ®æºï¼šNewsAPIã€Google News RSSã€Bing News
"""

import os
import json
import requests
import feedparser
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiSourceNewsFetcher:
    """å¤šæºæ–°é—»è·å–å™¨"""

    def __init__(self):
        # APIå¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        self.newsapi_key = os.getenv('NEWSAPI_KEY', '')
        self.bing_api_key = os.getenv('BING_NEWS_API_KEY', '')

        # æ¯ä¸ªå“ç‰Œè·å–çš„æ–°é—»æ•°é‡ï¼ˆå¯é…ç½®ï¼‰
        self.news_per_brand = int(os.getenv('NEWS_PER_BRAND', '10'))

        # å“ç‰Œåˆ—è¡¨
        self.brands = [
            'Eufy', 'Roborock', 'Dreame', 'Ecovacs',
            'Shark', 'Narwal', 'iRobot', 'Dyson'
        ]

        # æœç´¢å…³é”®è¯ï¼ˆå“ç‰Œ + robot vacuumï¼‰
        self.search_keywords = {
            'Eufy': 'Eufy robot vacuum',
            'Roborock': 'Roborock robot vacuum',
            'Dreame': 'Dreame robot vacuum',
            'Ecovacs': 'Ecovacs Deebot robot vacuum',
            'Shark': 'Shark robot vacuum',
            'Narwal': 'Narwal robot vacuum',
            'iRobot': 'iRobot Roomba',
            'Dyson': 'Dyson robot vacuum'
        }

    def fetch_from_newsapi(self, brand, keyword):
        """
        ä» NewsAPI.org è·å–æ–°é—»
        å…è´¹ç‰ˆï¼š100æ¬¡è¯·æ±‚/å¤©
        æ–‡æ¡£ï¼šhttps://newsapi.org/docs
        """
        if not self.newsapi_key:
            logger.warning("NewsAPI key not configured")
            return []

        try:
            url = "https://newsapi.org/v2/everything"
            params = {
                'apiKey': self.newsapi_key,
                'q': keyword,
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': min(self.news_per_brand * 2, 100),  # è·å–æ›´å¤šä»¥ä¾¿è¿‡æ»¤
                'from': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            }

            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])

                news_list = []
                for article in articles[:self.news_per_brand]:  # å–é…ç½®çš„æ•°é‡
                    news_list.append({
                        'brand': brand,
                        'title': article.get('title', ''),
                        'summary': article.get('description', '')[:200] + '...',
                        'source': article.get('source', {}).get('name', 'NewsAPI'),
                        'url': article.get('url', ''),
                        'date': article.get('publishedAt', '')[:10],
                        'image': article.get('urlToImage', '')
                    })

                logger.info(f"âœ… NewsAPI: {brand} - {len(news_list)} articles")
                return news_list

            elif response.status_code == 429:
                logger.warning(f"NewsAPI rate limit reached")
                return []

        except Exception as e:
            logger.error(f"NewsAPI error for {brand}: {e}")

        return []

    def fetch_from_google_news_rss(self, brand, keyword):
        """
        ä» Google News RSS è·å–æ–°é—»
        å®Œå…¨å…è´¹ï¼Œæ— éœ€APIå¯†é’¥
        """
        try:
            # Google News RSS URL
            rss_url = f"https://news.google.com/rss/search?q={keyword}&hl=en-US&gl=US&ceid=US:en"

            # è§£æRSS
            feed = feedparser.parse(rss_url)

            news_list = []
            for entry in feed.entries[:self.news_per_brand]:  # å–é…ç½®çš„æ•°é‡
                # æå–å‘å¸ƒæ—¥æœŸ
                pub_date = entry.get('published', '')
                try:
                    date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                    formatted_date = date_obj.strftime('%Y-%m-%d')
                except:
                    formatted_date = datetime.now().strftime('%Y-%m-%d')

                news_list.append({
                    'brand': brand,
                    'title': entry.get('title', ''),
                    'summary': entry.get('summary', '')[:200] + '...',
                    'source': 'Google News',
                    'url': entry.get('link', ''),
                    'date': formatted_date,
                    'image': ''
                })

            logger.info(f"âœ… Google News RSS: {brand} - {len(news_list)} articles")
            return news_list

        except Exception as e:
            logger.error(f"Google News RSS error for {brand}: {e}")

        return []

    def fetch_from_bing_news(self, brand, keyword):
        """
        ä» Bing News API è·å–æ–°é—»
        å…è´¹ç‰ˆï¼š3000æ¬¡/æœˆ
        """
        if not self.bing_api_key:
            logger.warning("Bing News API key not configured")
            return []

        try:
            url = "https://api.bing.microsoft.com/v7.0/news/search"
            headers = {'Ocp-Apim-Subscription-Key': self.bing_api_key}
            params = {
                'q': keyword,
                'count': self.news_per_brand,
                'mkt': 'en-US',
                'freshness': 'Month'
            }

            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                articles = data.get('value', [])

                news_list = []
                for article in articles:
                    news_list.append({
                        'brand': brand,
                        'title': article.get('name', ''),
                        'summary': article.get('description', '')[:200] + '...',
                        'source': article.get('provider', [{}])[0].get('name', 'Bing News'),
                        'url': article.get('url', ''),
                        'date': article.get('datePublished', '')[:10],
                        'image': article.get('image', {}).get('thumbnail', {}).get('contentUrl', '')
                    })

                logger.info(f"âœ… Bing News: {brand} - {len(news_list)} articles")
                return news_list

        except Exception as e:
            logger.error(f"Bing News error for {brand}: {e}")

        return []

    def fetch_brand_news(self, brand):
        """
        ä¸ºå•ä¸ªå“ç‰Œè·å–æ–°é—»ï¼ˆå°è¯•å¤šä¸ªæ•°æ®æºï¼‰
        ä¼˜å…ˆçº§ï¼šNewsAPI â†’ Google News RSS â†’ Bing News
        """
        keyword = self.search_keywords.get(brand, f'{brand} robot vacuum')
        news_list = []

        # å°è¯• NewsAPI
        if self.newsapi_key:
            news_list = self.fetch_from_newsapi(brand, keyword)
            if len(news_list) >= self.news_per_brand:
                return news_list

        # å¤‡é€‰ï¼šGoogle News RSSï¼ˆå…è´¹ï¼‰
        google_news = self.fetch_from_google_news_rss(brand, keyword)
        news_list.extend(google_news)
        if len(news_list) >= self.news_per_brand:
            return news_list[:self.news_per_brand]

        # å¤‡é€‰ï¼šBing News
        if self.bing_api_key and len(news_list) < self.news_per_brand:
            bing_news = self.fetch_from_bing_news(brand, keyword)
            news_list.extend(bing_news)

        # å»é‡ï¼ˆæ ¹æ®URLï¼‰
        seen_urls = set()
        unique_news = []
        for news in news_list:
            if news['url'] not in seen_urls:
                seen_urls.add(news['url'])
                unique_news.append(news)

        return unique_news[:self.news_per_brand]  # è¿”å›é…ç½®çš„æ•°é‡

    def fetch_all_brands(self):
        """è·å–æ‰€æœ‰å“ç‰Œçš„æ–°é—»"""
        all_news = []

        logger.info("="*70)
        logger.info("ğŸš€ å¼€å§‹è·å–å“ç‰Œæ–°é—»")
        logger.info("="*70)

        for idx, brand in enumerate(self.brands, 1):
            logger.info(f"\n[{idx}/{len(self.brands)}] è·å– {brand} æ–°é—»...")

            brand_news = self.fetch_brand_news(brand)
            all_news.extend(brand_news)

            logger.info(f"    âœ… è·å–åˆ° {len(brand_news)} æ¡æ–°é—»")

            # é¿å…é¢‘ç¹è¯·æ±‚
            if idx < len(self.brands):
                time.sleep(2)

        logger.info("\n" + "="*70)
        logger.info(f"âœ… æ€»è®¡è·å– {len(all_news)} æ¡æ–°é—»")
        logger.info("="*70)

        return all_news

    def save_to_json(self, news_data, output_file='../data/latest_news.json'):
        """ä¿å­˜æ–°é—»åˆ°JSONæ–‡ä»¶"""
        data = {
            'last_update': datetime.now().isoformat(),
            'total_news': len(news_data),
            'news': news_data
        }

        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ’¾ æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸ“° æœºå™¨äººå¸å°˜å™¨å“ç‰Œæ–°é—»è‡ªåŠ¨æŠ“å–å™¨")
    print("="*70)

    # æ£€æŸ¥APIé…ç½®
    newsapi_key = os.getenv('NEWSAPI_KEY')
    bing_key = os.getenv('BING_NEWS_API_KEY')

    print("\nğŸ”‘ APIé…ç½®æ£€æŸ¥:")
    print(f"  NewsAPI: {'âœ… å·²é…ç½®' if newsapi_key else 'âš ï¸  æœªé…ç½®ï¼ˆå°†ä½¿ç”¨Google News RSSï¼‰'}")
    print(f"  Bing News: {'âœ… å·²é…ç½®' if bing_key else 'âš ï¸  æœªé…ç½®'}")

    if not newsapi_key and not bing_key:
        print("\nğŸ’¡ æç¤º: æœªé…ç½®ä»»ä½•APIå¯†é’¥ï¼Œå°†ä½¿ç”¨å…è´¹çš„Google News RSS")
        print("   Google News RSSå®Œå…¨å…è´¹ï¼Œä½†å¯èƒ½è·å–é€Ÿåº¦è¾ƒæ…¢")

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    fetcher = MultiSourceNewsFetcher()

    # è·å–æ‰€æœ‰æ–°é—»
    all_news = fetcher.fetch_all_brands()

    # ä¿å­˜åˆ°æ–‡ä»¶
    fetcher.save_to_json(all_news)

    # ç»Ÿè®¡æ¯ä¸ªå“ç‰Œçš„æ–°é—»æ•°é‡
    print("\nğŸ“Š å“ç‰Œæ–°é—»ç»Ÿè®¡:")
    brand_counts = {}
    for news in all_news:
        brand = news['brand']
        brand_counts[brand] = brand_counts.get(brand, 0) + 1

    for brand, count in sorted(brand_counts.items()):
        status = "âœ…" if count >= 5 else "âš ï¸ "
        print(f"  {status} {brand}: {count} æ¡")

    print("\n" + "="*70)
    print("âœ… æ–°é—»æŠ“å–å®Œæˆï¼")
    print("="*70)
    print()


if __name__ == "__main__":
    main()
