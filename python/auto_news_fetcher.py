"""
Automatic News Fetcher - Multi-source News Crawler
æ¯æ—¥è‡ªåŠ¨è·å–8ä¸ªå“ç‰Œçš„æœ€æ–°èµ„è®¯ï¼ˆæ¯ä¸ªå“ç‰Œè‡³å°‘5æ¡ï¼‰
æ”¯æŒå¤šä¸ªæ•°æ®æºï¼šNewsAPIã€Google News RSSã€Bing News
"""

import os
import json
import requests
import feedparser
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import time
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiSourceNewsFetcher:
    """å¤šæºæ–°é—»è·å–å™¨"""

    def __init__(self):
        # APIå¯†é’¥ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        self.newsapi_key = os.getenv('NEWSAPI_KEY', '')
        self.bing_api_key = os.getenv('BING_NEWS_API_KEY', '')

        # æ¯ä¸ªå“ç‰Œè·å–çš„æ–°é—»æ•°é‡ï¼ˆå¯é…ç½®ï¼‰
        self.news_per_brand = int(os.getenv('NEWS_PER_BRAND', '10'))

        # å“ç‰Œåˆ—è¡¨
        self.brands = [
            'Eufy', 'Roborock', 'Dreame', 'Ecovacs',
            'Shark', 'Narwal', 'iRobot', 'Dyson'
        ]

        # æœç´¢å…³é”®è¯ï¼ˆå“ç‰Œ + robot vacuumï¼‰
        self.search_keywords = {
            'Eufy': 'Eufy robot vacuum',
            'Roborock': 'Roborock robot vacuum',
            'Dreame': 'Dreame robot vacuum',
            'Ecovacs': 'Ecovacs Deebot robot vacuum',
            'Shark': 'Shark robot vacuum',
            'Narwal': 'Narwal robot vacuum',
            'iRobot': 'iRobot Roomba',
            'Dyson': 'Dyson robot vacuum'
        }

        # æ’é™¤çš„åŸŸåï¼ˆå“ç‰Œå®˜ç½‘ï¼‰
        self.excluded_domains = [
            'eufy.com', 'eufylife.com',
            'roborock.com', 'us.roborock.com',
            'dreame.com', 'dreametech.com',
            'ecovacs.com',
            'sharkclean.com', 'sharkninja.com',
            'narwal.com',
            'irobot.com',
            'dyson.com',
            'anker.com', 'ankermake.com'  # Eufyæ¯å…¬å¸
        ]

        # ä¼˜å…ˆçš„ç§‘æŠ€åª’ä½“æ¥æº
        self.preferred_sources = [
            'CNET', 'Tom\'s Guide', 'The Verge', 'TechCrunch',
            'Engadget', 'Android Authority', 'Digital Trends',
            'PCMag', 'TechRadar', 'Mashable', 'Wired',
            'The Guardian', 'Forbes', 'BBC', 'CNN',
            'Vacuum Wars', 'Robot Reviews'
        ]

    def is_excluded_url(self, url):
        """æ£€æŸ¥URLæ˜¯å¦æ¥è‡ªå“ç‰Œå®˜ç½‘ï¼ˆéœ€è¦æ’é™¤ï¼‰"""
        url_lower = url.lower()
        for domain in self.excluded_domains:
            if domain in url_lower:
                return True
        return False

    def is_preferred_source(self, source_name):
        """æ£€æŸ¥æ˜¯å¦æ˜¯ä¼˜å…ˆçš„ç§‘æŠ€åª’ä½“"""
        for preferred in self.preferred_sources:
            if preferred.lower() in source_name.lower():
                return True
        return False

    def fetch_from_newsapi(self, brand, keyword):
        """
        ä» NewsAPI.org è·å–æ–°é—»
        å…è´¹ç‰ˆï¼š100æ¬¡è¯·æ±‚/å¤©
        æ–‡æ¡£ï¼šhttps://newsapi.org/docs
        """
        if not self.newsapi_key:
            logger.warning("NewsAPI key not configured")
            return []

        try:
            # æ¸…ç†å…³é”®è¯
            clean_keyword = keyword.strip().replace('\t', ' ').replace('\n', ' ')

            url = "https://newsapi.org/v2/everything"
            params = {
                'apiKey': self.newsapi_key,
                'q': clean_keyword,
                'language': 'en',
                'sortBy': 'publishedAt',
                'pageSize': min(self.news_per_brand * 2, 100),  # è·å–æ›´å¤šä»¥ä¾¿è¿‡æ»¤
                'from': (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
            }

            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])

                news_list = []
                for article in articles:
                    article_url = article.get('url', '')
                    source_name = article.get('source', {}).get('name', '')

                    # è·³è¿‡å®˜ç½‘æ–°é—»
                    if self.is_excluded_url(article_url):
                        continue

                    news_list.append({
                        'brand': brand,
                        'title': article.get('title', ''),
                        'summary': article.get('description', '')[:200] + '...',
                        'source': source_name,
                        'url': article_url,
                        'date': article.get('publishedAt', '')[:10],
                        'image': article.get('urlToImage', ''),
                        'is_preferred': self.is_preferred_source(source_name)
                    })

                    # è·å–è¶³å¤Ÿæ•°é‡ååœæ­¢
                    if len(news_list) >= self.news_per_brand:
                        break

                logger.info(f"âœ… NewsAPI: {brand} - {len(news_list)} articles")
                return news_list

            elif response.status_code == 429:
                logger.warning(f"NewsAPI rate limit reached")
                return []

        except Exception as e:
            logger.error(f"NewsAPI error for {brand}: {e}")

        return []

    def extract_real_url_from_google(self, google_url):
        """
        ä»Google Newsé‡å®šå‘é“¾æ¥ä¸­æå–çœŸå®URL
        æ–¹æ³•1ï¼šBase64è§£ç 
        æ–¹æ³•2ï¼šHTTPè¯·æ±‚è·Ÿéšé‡å®šå‘
        """
        import base64
        import re

        # å¦‚æœä¸æ˜¯Google Newsé‡å®šå‘é“¾æ¥ï¼Œç›´æ¥è¿”å›
        if 'news.google.com/rss/articles/' not in google_url:
            return google_url

        # æ–¹æ³•1ï¼šBase64è§£ç æå–çœŸå®URL
        try:
            # æå–ç¼–ç éƒ¨åˆ† (CBMi... æˆ– CAIi...)
            match = re.search(r'/articles/(CBMi[A-Za-z0-9_-]+|CAIi[A-Za-z0-9_-]+)', google_url)
            if match:
                encoded = match.group(1)
                # å»æ‰å‰ç¼€ (CBMi æˆ– CAIi)
                encoded_data = encoded[4:]

                # Base64è§£ç 
                try:
                    # æ·»åŠ padding
                    padding = (4 - len(encoded_data) % 4) % 4
                    encoded_data += '=' * padding

                    decoded = base64.urlsafe_b64decode(encoded_data)
                    decoded_str = decoded.decode('utf-8', errors='ignore')

                    # æŸ¥æ‰¾httpé“¾æ¥ï¼ˆæ”¹è¿›çš„æ­£åˆ™ï¼‰
                    url_match = re.search(r'https?://[a-zA-Z0-9\-._~:/?#\[\]@!$&\'()*+,;=%]+', decoded_str)
                    if url_match:
                        real_url = url_match.group(0)
                        # æ¸…ç†URLæœ«å°¾çš„ç‰¹æ®Šå­—ç¬¦
                        real_url = re.sub(r'[\x00-\x1f]+.*$', '', real_url)
                        # ç§»é™¤æœ«å°¾çš„åƒåœ¾å­—ç¬¦
                        real_url = real_url.rstrip('\x00\x01\x02\x03\x08\x10\x12')
                        logger.info(f"âœ… Base64è§£ç æˆåŠŸ: {real_url[:60]}...")
                        return real_url
                except Exception as decode_error:
                    logger.debug(f"Base64è§£ç å¤±è´¥: {decode_error}")
        except Exception as e:
            logger.debug(f"Base64æ–¹æ³•å¤±è´¥: {e}")

        # æ–¹æ³•2ï¼šHTTPè¯·æ±‚è·Ÿéšé‡å®šå‘ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        try:
            logger.info(f"å°è¯•HTTPé‡å®šå‘è·Ÿè¸ª...")
            response = requests.head(google_url, allow_redirects=True, timeout=5)
            if response.url and response.url != google_url:
                logger.info(f"âœ… HTTPé‡å®šå‘æˆåŠŸ: {response.url[:60]}...")
                return response.url
        except Exception as e:
            logger.debug(f"HTTPé‡å®šå‘å¤±è´¥: {e}")

        # å¦‚æœä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›åŸURL
        logger.warning(f"âš ï¸  URLè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é“¾æ¥: {google_url[:60]}...")
        return google_url

    def fetch_from_google_news_rss(self, brand, keyword):
        """
        ä» Google News RSS è·å–æ–°é—»
        å®Œå…¨å…è´¹ï¼Œæ— éœ€APIå¯†é’¥
        """
        try:
            # æ¸…ç†å…³é”®è¯ï¼šç§»é™¤éASCIIå­—ç¬¦ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦
            import urllib.parse
            clean_keyword = keyword.strip().replace('\t', ' ').replace('\n', ' ')
            # URLç¼–ç 
            encoded_keyword = urllib.parse.quote(clean_keyword)

            # Google News RSS URL
            rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=en-US&gl=US&ceid=US:en"

            # è§£æRSS
            feed = feedparser.parse(rss_url)

            news_list = []
            for entry in feed.entries:
                entry_url = entry.get('link', '')

                # è·³è¿‡å®˜ç½‘æ–°é—»
                if self.is_excluded_url(entry_url):
                    continue

                # æå–å‘å¸ƒæ—¥æœŸ
                pub_date = entry.get('published', '')
                try:
                    date_obj = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                    formatted_date = date_obj.strftime('%Y-%m-%d')
                except:
                    formatted_date = datetime.now().strftime('%Y-%m-%d')

                # å°è¯•ä»titleä¸­æå–çœŸå®æ¥æº
                title = entry.get('title', '')
                source_name = 'Google News'
                if ' - ' in title:
                    # Google Newsçš„æ ‡é¢˜æ ¼å¼é€šå¸¸æ˜¯ "æ ‡é¢˜ - æ¥æº"
                    source_name = title.split(' - ')[-1].strip()

                # æ¸…ç†å’ŒéªŒè¯URL
                clean_url = entry_url.strip()
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„http/httpsé“¾æ¥
                if not clean_url.startswith('http'):
                    logger.warning(f"Invalid URL for {brand}: {clean_url}")
                    continue

                # æå–Google Newsé‡å®šå‘ä¸­çš„çœŸå®URL
                real_url = self.extract_real_url_from_google(clean_url)

                # ç¡®ä¿URLä¸æ˜¯GitHubä»£ç é“¾æ¥
                if 'github.com' in real_url.lower() and '/blob/' in real_url.lower():
                    logger.warning(f"Skipping GitHub code URL: {real_url}")
                    continue

                news_list.append({
                    'brand': brand,
                    'title': title.split(' - ')[0] if ' - ' in title else title,  # ç§»é™¤æ¥æºéƒ¨åˆ†
                    'summary': entry.get('summary', '')[:200] + '...',
                    'source': source_name,
                    'url': real_url,  # ä½¿ç”¨æå–çš„çœŸå®URL
                    'date': formatted_date,
                    'image': '',
                    'is_preferred': self.is_preferred_source(source_name)
                })

                # è·å–è¶³å¤Ÿæ•°é‡ååœæ­¢
                if len(news_list) >= self.news_per_brand:
                    break

            logger.info(f"âœ… Google News RSS: {brand} - {len(news_list)} articles")
            return news_list

        except Exception as e:
            logger.error(f"Google News RSS error for {brand}: {e}")

        return []

    def fetch_from_bing_news(self, brand, keyword):
        """
        ä» Bing News API è·å–æ–°é—»
        å…è´¹ç‰ˆï¼š3000æ¬¡/æœˆ
        """
        if not self.bing_api_key:
            logger.warning("Bing News API key not configured")
            return []

        try:
            # æ¸…ç†å…³é”®è¯
            clean_keyword = keyword.strip().replace('\t', ' ').replace('\n', ' ')

            url = "https://api.bing.microsoft.com/v7.0/news/search"
            headers = {'Ocp-Apim-Subscription-Key': self.bing_api_key}
            params = {
                'q': clean_keyword,
                'count': self.news_per_brand,
                'mkt': 'en-US',
                'freshness': 'Month'
            }

            response = requests.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                articles = data.get('value', [])

                news_list = []
                for article in articles:
                    article_url = article.get('url', '')
                    source_name = article.get('provider', [{}])[0].get('name', 'Bing News')

                    # è·³è¿‡å®˜ç½‘æ–°é—»
                    if self.is_excluded_url(article_url):
                        continue

                    news_list.append({
                        'brand': brand,
                        'title': article.get('name', ''),
                        'summary': article.get('description', '')[:200] + '...',
                        'source': source_name,
                        'url': article_url,
                        'date': article.get('datePublished', '')[:10],
                        'image': article.get('image', {}).get('thumbnail', {}).get('contentUrl', ''),
                        'is_preferred': self.is_preferred_source(source_name)
                    })

                    # è·å–è¶³å¤Ÿæ•°é‡ååœæ­¢
                    if len(news_list) >= self.news_per_brand:
                        break

                logger.info(f"âœ… Bing News: {brand} - {len(news_list)} articles")
                return news_list

        except Exception as e:
            logger.error(f"Bing News error for {brand}: {e}")

        return []

    def fetch_brand_news(self, brand):
        """
        ä¸ºå•ä¸ªå“ç‰Œè·å–æ–°é—»ï¼ˆå°è¯•å¤šä¸ªæ•°æ®æºï¼‰
        ä¼˜å…ˆçº§ï¼šNewsAPI â†’ Google News RSS â†’ Bing News
        """
        keyword = self.search_keywords.get(brand, f'{brand} robot vacuum')
        news_list = []

        # å°è¯• NewsAPI
        if self.newsapi_key:
            news_list = self.fetch_from_newsapi(brand, keyword)
            if len(news_list) >= self.news_per_brand:
                return news_list

        # å¤‡é€‰ï¼šGoogle News RSSï¼ˆå…è´¹ï¼‰
        google_news = self.fetch_from_google_news_rss(brand, keyword)
        news_list.extend(google_news)
        if len(news_list) >= self.news_per_brand:
            return news_list[:self.news_per_brand]

        # å¤‡é€‰ï¼šBing News
        if self.bing_api_key and len(news_list) < self.news_per_brand:
            bing_news = self.fetch_from_bing_news(brand, keyword)
            news_list.extend(bing_news)

        # å»é‡ï¼ˆæ ¹æ®URLï¼‰
        seen_urls = set()
        unique_news = []
        for news in news_list:
            if news['url'] not in seen_urls:
                seen_urls.add(news['url'])
                unique_news.append(news)

        # ä¼˜å…ˆæ’åºï¼šä¼˜å…ˆåª’ä½“çš„æ–°é—»æ’åœ¨å‰é¢
        unique_news.sort(key=lambda x: (
            not x.get('is_preferred', False),  # ä¼˜å…ˆåª’ä½“æ’å‰é¢ï¼ˆFalse < Trueï¼Œå–ååTrue < Falseï¼‰
            x.get('date', '')  # æŒ‰æ—¥æœŸé™åº
        ), reverse=True)

        return unique_news[:self.news_per_brand]  # è¿”å›é…ç½®çš„æ•°é‡

    def fetch_all_brands(self):
        """è·å–æ‰€æœ‰å“ç‰Œçš„æ–°é—»"""
        all_news = []

        logger.info("="*70)
        logger.info("ğŸš€ å¼€å§‹è·å–å“ç‰Œæ–°é—»")
        logger.info("="*70)

        for idx, brand in enumerate(self.brands, 1):
            logger.info(f"\n[{idx}/{len(self.brands)}] è·å– {brand} æ–°é—»...")

            brand_news = self.fetch_brand_news(brand)
            all_news.extend(brand_news)

            logger.info(f"    âœ… è·å–åˆ° {len(brand_news)} æ¡æ–°é—»")

            # é¿å…é¢‘ç¹è¯·æ±‚
            if idx < len(self.brands):
                time.sleep(2)

        logger.info("\n" + "="*70)
        logger.info(f"âœ… æ€»è®¡è·å– {len(all_news)} æ¡æ–°é—»")
        logger.info("="*70)

        return all_news

    def save_to_json(self, news_data, output_file='../data/latest_news.json'):
        """ä¿å­˜æ–°é—»åˆ°JSONæ–‡ä»¶"""
        data = {
            'last_update': datetime.now().isoformat(),
            'total_news': len(news_data),
            'news': news_data
        }

        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        logger.info(f"\nğŸ’¾ æ–°é—»æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸ“° æœºå™¨äººå¸å°˜å™¨å“ç‰Œæ–°é—»è‡ªåŠ¨æŠ“å–å™¨")
    print("="*70)

    # æ£€æŸ¥APIé…ç½®
    newsapi_key = os.getenv('NEWSAPI_KEY')
    bing_key = os.getenv('BING_NEWS_API_KEY')

    print("\nğŸ”‘ APIé…ç½®æ£€æŸ¥:")
    print(f"  NewsAPI: {'âœ… å·²é…ç½®' if newsapi_key else 'âš ï¸  æœªé…ç½®ï¼ˆå°†ä½¿ç”¨Google News RSSï¼‰'}")
    print(f"  Bing News: {'âœ… å·²é…ç½®' if bing_key else 'âš ï¸  æœªé…ç½®'}")

    if not newsapi_key and not bing_key:
        print("\nğŸ’¡ æç¤º: æœªé…ç½®ä»»ä½•APIå¯†é’¥ï¼Œå°†ä½¿ç”¨å…è´¹çš„Google News RSS")
        print("   Google News RSSå®Œå…¨å…è´¹ï¼Œä½†å¯èƒ½è·å–é€Ÿåº¦è¾ƒæ…¢")

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    fetcher = MultiSourceNewsFetcher()

    # è·å–æ‰€æœ‰æ–°é—»
    all_news = fetcher.fetch_all_brands()

    # ä¿å­˜åˆ°æ–‡ä»¶
    fetcher.save_to_json(all_news)

    # ç»Ÿè®¡æ¯ä¸ªå“ç‰Œçš„æ–°é—»æ•°é‡
    print("\nğŸ“Š å“ç‰Œæ–°é—»ç»Ÿè®¡:")
    brand_counts = {}
    for news in all_news:
        brand = news['brand']
        brand_counts[brand] = brand_counts.get(brand, 0) + 1

    for brand, count in sorted(brand_counts.items()):
        status = "âœ…" if count >= 5 else "âš ï¸ "
        print(f"  {status} {brand}: {count} æ¡")

    print("\n" + "="*70)
    print("âœ… æ–°é—»æŠ“å–å®Œæˆï¼")
    print("="*70)
    print()


if __name__ == "__main__":
    main()
